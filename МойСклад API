Скрипт Request для вывода отчета по складам МойСклад API


# -*- coding: utf-8 -*-
"""AllScripts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TRBXpzwr2KKP0qwxorAiTclW50NedIo2
"""

import pandas as pd
import json
import io
import time
import pandas
from pandas import json_normalize
import requests
import ast
from datetime import datetime
import pandas as pd
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
import pandas as pd
import json
import io
import time
import pandas
from pandas import json_normalize
import requests
import ast
from datetime import datetime
import pandas as pd
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
startTime = datetime.now()

all_orders = []
offset = 0
while True:

    #url = 'https://api.moysklad.ru/api/remap/1.2/entity/product/metadata/attributes/<id>=12dd3ad1-bd03-11ed-0a80-04df008e3fb8'
    #url = 'https://api.moysklad.ru/api/remap/1.2/report/stock/all/current?stockType=quantity&include=zeroLines'
    #url = 'https://api.moysklad.ru/api/remap/1.2/report/stock/all'

    #url = f"https://api.moysklad.ru/api/remap/1.2/report/stock/all?limit=1000&offset={offset}"
    url = f"https://api.moysklad.ru/api/remap/1.2/report/stock/all?offset={offset}&limit=1000"
   # url = f"https://api.moysklad.ru/api/remap/1.2/entity/assortment?limit=1000&offset={offset}"
    api_key = '91747401152ee277431c2d47115df852fdb8a1a6'
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "User-Agent": "python-requests/2.31.0"
    }

    response = requests.get(url, headers=headers) #, params=params)
    result = response.json()

    all_orders.extend(result['rows'])
    # for i in range(len(result['rows'])):
    #     href=result['rows'][i]['meta']['href']
    #     res = requests.get(href, headers=headers) #, params=params)
    #     rows = res.json()
    #     all_orders.extend(rows)

    if offset > int(result['meta']['size']):

        break
    #print(offset)
    print(result['meta']['offset'])

    offset += 1000  # Move to next page
    time.sleep(5)  # Add delay to avoid rate limits
df2 = pd.json_normalize(all_orders)

df2['folder.pathName'].unique()

selected_columns=['stock', 'inTransit', 'reserve', 'quantity', 'name', 'code', 'article',
       'price', 'salePrice', 'externalCode', 'stockDays', 'meta.href','folder.name','folder.pathName',]

df2.columns

#df_filtered = df2[~df2['folder.pathName'].str.contains('Архив', na=False)]

#result = df_filtered[selected_columns].reset_index(drop=True)

result = df2[df2['folder.pathName'].str.contains('Lamoda')][selected_columns].reset_index(drop=True)

result['folder.pathName'].unique()

result['folder.pathName'].unique()

result.iloc[34]

ACCESS_TOKEN = ''

headers = {
    "Authorization": f"Bearer {ACCESS_TOKEN}",
    "Accept-Encoding": "gzip"
}
def fetch_order(url, retries=3, timeout=10):
    #url = f'https://api.moysklad.ru/api/remap/1.2/entity/product/{moysklad_id}'
    #url = f'https://api.moysklad.ru/api/remap/1.2/report/byoperations/stock?filter=assortment=https://api.moysklad.ru/api/remap/1.2/entity/product/{moysklad_id}'
    #url = f'https://api.moysklad.ru/api/remap/1.2/report/stock/byoperation?operation.id={moysklad_id}'

    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            result= response.json()
            return result['attributes']

            # if response.status_code == 200:
            #     result= response.json()
            #     return result['attributes']
            # else:
            #     return {"error": f"Status {response.status_code}: {response.text}"}
            #    # continue
        except requests.exceptions.Timeout:
            print(f"Timeout for ID {url}, attempt {attempt + 1} of {retries}")
        except requests.exceptions.RequestException as e:
            print(f"Error for ID {url}: {e}")
            return None  # Stop retrying if it's a non-timeout error

        time.sleep(4)  # Wait before retrying

    print(f"Failed after {retries} attempts for ID {url}")
    return None

# #API_URL = "https://api.moysklad.ru/api/remap/1.2"
# ACCESS_TOKEN = ''

# headers = {
#     "Authorization": f"Bearer {ACCESS_TOKEN}",
#     "Accept-Encoding": "gzip"
# }
# def fetch_order(url):
#     time.sleep(5)

#     try:
#         response = requests.get(url, headers=headers, verify=False)
#         if response.status_code == 200:
#             result= response.json()

#             return result['attributes']

#         else:
#             return {"error": f"Status {response.status_code}: {response.text}"}
#     except Exception as e:
#         return {"error": str(e)}

expanded_list = []
order_details=result['meta.href'].apply(fetch_order)
expanded_list.extend(order_details)


data = []

len(expanded_list)

data = []

# Предполагаем, что csvFile['meta.href'] и expanded_list имеют одинаковую длину
for idx, attributes in enumerate(expanded_list):
    # Проверяем, что attributes является списком (а не строкой или ошибкой)
    if isinstance(attributes, list):
        attribute_dict = {}
        attribute_dict['meta.href'] = result['meta.href'][idx]  # Добавляем meta.href из csvFile['meta.href']

        for attr in attributes:
            # Проверяем, что attr является словарем и содержит ключи 'name' и 'value'
            if isinstance(attr, dict) and 'name' in attr and 'value' in attr:
                name = attr['name']
                value = attr['value']

                if isinstance(value, dict) and 'name' in value:
                    attribute_dict[name] = value['name']
                else:
                    attribute_dict[name] = value
            else:
                print(f"Skipping invalid attribute: {attr}")
        data.append(attribute_dict)
    else:
        print(f"Skipping invalid attributes for row {idx}: {attributes}")

df_2 = pd.DataFrame(data)

merged_df = pd.merge(result, df_2, on='meta.href', how='outer')

merged_df.columns

selected_col=['stock', 'inTransit', 'reserve', 'quantity', 'name', 'article',
       'price', 'salePrice',  'stockDays', 'meta.href',
       'folder.name', 'folder.pathName', 'Артикул', 'Состав', 'Адрес',
       'Поставщик', 'Цвет', 'Размер', 'Технический размер', 'Бренд',
       'Коэффициент сложности сборки',  'Название для бирки',
       'Модель и цвет', 'Категория', 'Дата изготовления',
        'Сток', 'MOQ сток', 'СРОК сток', 'Отшив',
       'MOQ отшив', 'СРОК отшив', 'Откуда закупаем', 'Кто заводил',
       'Цена доставки', 'Налог ИП',
       'Дата регистрации СС', 'Дата окончания действия СС',
        'Вшивник',
       'Процент выкупа',
       'Дата регистрации ДС', 'Дата окончания действия ДС']

result = merged_df[selected_col].reset_index(drop=True)

result

SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']

creds = Credentials.from_service_account_file('/content/halogen-pier-451511-d0-7b522bfe360b.json', scopes=SCOPES)
client = gspread.authorize(creds)

# Создать новую таблицу
#spreadsheet = client.create("Новая таблица")

# Или открыть существующую таблицу по имени
spreadsheet = client.open("Игорь / аналитика амазонка")

# Выберите первый лист (или любой другой по имени) https://docs.google.com/spreadsheets/d/1ApnRhKoWfwe66idRprHOulvC3QwDmoakWZ_B_TTmnhQ/edit?gid=496060326#gid=496060326
#sheet = spreadsheet.worksheet('496060326')
# sheet_gid = "496060326"  # Например, "0" для первого листа

# # Найти лист по gid
# sheet = None
# for worksheet in spreadsheet.worksheets():
#     if str(worksheet.id) == sheet_gid:
#         sheet = worksheet
#         break

# Преобразуем DataFrame в список списков
#values = [finaldf.columns.values.tolist()] + finaldf.values.tolist()
worksheet1 = spreadsheet.worksheet("Мой Склад API")

worksheet1.clear()
set_with_dataframe(worksheet=worksheet1, dataframe=result, include_index=False,
include_column_header=True, resize=True)
# # Вставляем данные в Google Sheets
# sheet.update('A1', values)
# #sheet.update(range_name='A1', values=values)

print("Данные успешно вставлены в Google Sheets!")





# # #API_URL = "https://api.moysklad.ru/api/remap/1.2"
# # ACCESS_TOKEN = ''

# # headers = {
# #     "Authorization": f"Bearer {ACCESS_TOKEN}",
# #     "Accept-Encoding": "gzip"
# # }
# # def fetch_order(url):

# #     try:
# #         response = requests.get(url, headers=headers)
# #         if response.status_code == 200:
# #             result= response.json()

# #             return result['attributes']

# #         else:
# #             return {"error": f"Status {response.status_code}: {response.text}"}
# #     except Exception as e:
# #         return {"error": str(e)}

# # expanded_list = []
# # order_details=result['meta.href'].apply(fetch_order)
# # expanded_list.extend(order_details)


# # data = []


# # for attributes in expanded_list:
# #     attribute_dict = {}
# #     for attr in attributes:
# #         name = attr['name']
# #         value = attr['value']


# #         if isinstance(value, dict) and 'name' in value:
# #             attribute_dict[name] = value['name']
# #         else:
# #             attribute_dict[name] = value
# #     data.append(attribute_dict)

# # df_2 = pd.DataFrame(data)

# for idx, attributes in enumerate(expanded_list):
#     attribute_dict = {}
#     attribute_dict['meta.href'] = result['meta.href'][idx]

#     for attr in attributes:
#         name = attr['name']
#         value = attr['value']

#         if isinstance(value, dict) and 'name' in value:
#             attribute_dict[name] = value['name']
#         else:
#             attribute_dict[name] = value
#     data.append(attribute_dict)

# dffinal = pd.DataFrame(data)









with pd.ExcelWriter('dfd.xlsx') as writer:
    # writing to the 'Employee' sheet
    merged_df.to_excel(writer, sheet_name='mois3', index=False)

